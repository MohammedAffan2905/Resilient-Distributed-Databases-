{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "Bfu96bUpy1OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "RhbaMRHSzCqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfY9vo1POLdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating RDD from LIST**"
      ],
      "metadata": {
        "id": "h0c6L1AiOJda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "from pyspark import SparkContext\n",
        "\n",
        "# Creating a SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Basic Example\")"
      ],
      "metadata": {
        "id": "x0SAW9QGOmq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Check if SparkContext exists\n",
        "try:\n",
        "    sc = SparkContext()\n",
        "    spark = SparkSession(sc)\n",
        "except ValueError:\n",
        "    from pyspark.sql import SparkSession\n",
        "    spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"RDD Basic Example\") \\\n",
        "        .getOrCreate()\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "# Creating an RDD from a list of numbers\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "FavVNPI5PCkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an RDD from a list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "ISQepw2IOCoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ** RDD Transformations**"
      ],
      "metadata": {
        "id": "VhxPjCJkQ0OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation: Squaring each element in the RDD\n",
        "rdd_squared = rdd.map(lambda x: x*x)\n",
        "\n",
        "\n",
        "# Transformation: Filtering even numbers\n",
        "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n"
      ],
      "metadata": {
        "id": "rlueLT2lOC-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Action: Collecting squared values\n",
        "squared_values = rdd_squared.collect()\n",
        "print(\"Squared Values:\", squared_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKzHdNaOODJT",
        "outputId": "1c943055-442a-46b3-c060-e338253bbd51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared Values: [1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action: Collecting filtering even values\n",
        "even_values = rdd_even.collect()\n",
        "print(\"Even Values:\", even_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6--vNRzRjJd",
        "outputId": "fa8713e1-87e5-4550-8077-eaa40415440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even Values: [2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action: Reducing to find the sum\n",
        "sum_of_values = rdd.reduce(lambda x, y: x + y)\n",
        "print(\"Sum of Values:\", sum_of_values)\n",
        "\n",
        "# Action: Collecting even numbers\n",
        "even_numbers = rdd_even.collect()\n",
        "print(\"Even Numbers:\", even_numbers)\n",
        "\n",
        "# Stopping the SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51EPiTzWSl18",
        "outputId": "bfeaf2de-ac1a-42b5-83fb-e125a082ea83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum of Values: 15\n",
            "Even Numbers: [2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RDD Narrow Transformations**\n",
        "In Apache Spark, narrow transformations are operations that are performed on individual partitions of an RDD without shuffling or redistributing the data across the cluster. These transformations are executed within the partitions, making them more efficient than wide transformations, which may require data shuffling across the cluster.\n",
        "\n",
        "Here are some common narrow transformations in Spark RDDs along with examples:"
      ],
      "metadata": {
        "id": "y_JoInXdTQ_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. map(func):**\n",
        "\n",
        "Applies a function to each element of the RDD and returns a new RDD.\n",
        "The function can be any Python function or lambda expression."
      ],
      "metadata": {
        "id": "WWsrV9LYTt1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation: Squaring each element in the RDD\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd_squared = rdd.map(lambda x: x*x)\n",
        "# Action: Collecting squared values\n",
        "squared_values = rdd_squared.collect()\n",
        "print(\"Squared Values:\", squared_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSEtua8dTQIE",
        "outputId": "521fb4d5-1442-4452-8ac5-de80d42ae2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared Values: [1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.filter(func):**\n",
        "\n",
        "Returns a new RDD containing only the elements that satisfy the given predicate (function).\n",
        "The predicate function should return True or False."
      ],
      "metadata": {
        "id": "YdL4Fmc3UIWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd_even = rdd.filter(lambda x: x % 2 == 0)\n",
        "# Action: filtering the even numbers\n",
        "even_values = rdd_even.collect()\n",
        "print(\"Squared Values:\", even_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0NAsbvRULw0",
        "outputId": "56b91865-770f-44ec-d200-6218e9786901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared Values: [2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.flatMap(func):**\n",
        "\n",
        "Similar to map, but each input item can be mapped to 0 or more output items.\n",
        "The output is flattened into a single RDD."
      ],
      "metadata": {
        "id": "9RzwZWSsVdez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([\"Hello World\", \"Spark is awesome\"])\n",
        "rdd_flat = rdd.flatMap(lambda line: line.split())\n",
        "#Action:\n",
        "rdd_flat_values=rdd_flat.collect()\n",
        "print(rdd_flat_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnyi8--GVf1E",
        "outputId": "a2652c87-1925-4460-9982-71ffcc96a668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', 'Spark', 'is', 'awesome']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Narrow Transformations Example\")\n",
        "\n",
        "# Create an RDD from a list of sentences\n",
        "sentences = [\"Hello world\", \"Spark is awesome\"]\n",
        "rdd = sc.parallelize(sentences)\n",
        "\n",
        "# Apply flatMap transformation to split words\n",
        "words_rdd = rdd.flatMap(lambda sentence: sentence.split())\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "print(\"Words RDD:\", words_rdd.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXSt1-Cynpei",
        "outputId": "54cc8f9a-cad5-49a1-bcac-bc2d215971d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RDD: ['Hello world', 'Spark is awesome']\n",
            "Words RDD: ['Hello', 'world', 'Spark', 'is', 'awesome']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.mapPartitions(func):**\n",
        "\n",
        "Similar to map, but operates on each partition of the RDD independently.\n",
        "The function is applied to an iterator of elements within each partition."
      ],
      "metadata": {
        "id": "wvEyAoLYVmbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)  # Creating RDD with 2 partitions\n",
        "def map_partition_func(iterator):\n",
        "    return [sum(iterator)]\n",
        "rdd_mapped = rdd.mapPartitions(map_partition_func)\n",
        "#Action:\n",
        "rdd_mapped_values=rdd_mapped.collect()\n",
        "print(rdd_mapped_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDicDaU2Vo7j",
        "outputId": "75b26761-1f1a-4295-9c10-f4f33841829c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.mapPartitionsWithIndex(func):**\n",
        "\n",
        "Similar to mapPartitions, but also provides the index of the partition.\n",
        "Useful for tasks that require knowing the partition index."
      ],
      "metadata": {
        "id": "x9qL32SWVvS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)  # Creating RDD with 2 partitions\n",
        "def map_partition_func_with_index(partition_index, iterator):\n",
        "    return [(partition_index, sum(iterator))]\n",
        "rdd_mapped_with_index = rdd.mapPartitionsWithIndex(map_partition_func_with_index)\n",
        "#Action:\n",
        "rdd_mapped_with_index=rdd_mapped_with_index.collect()\n",
        "print(rdd_mapped_with_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc0YrAPZVuwT",
        "outputId": "f29d5b2c-65e8-48d2-c5b3-0a3160c8f042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 3), (1, 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "zVcP4yelYZrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: mapPartitions\n",
        "def map_partition_func(iterator):\n",
        "    return [sum(iterator)]\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)  # Creating RDD with 2 partitions\n",
        "rdd_mapped = rdd.mapPartitions(map_partition_func)\n",
        "\n",
        "# Action: Collecting values after mapPartitions transformation\n",
        "mapped_values = rdd_mapped.collect()\n",
        "print(\"Mapped Values (mapPartitions):\", mapped_values)\n",
        "\n",
        "# Example: mapPartitionsWithIndex\n",
        "def map_partition_func_with_index(partition_index, iterator):\n",
        "    return [(partition_index, sum(iterator))]\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5], 2)  # Creating RDD with 2 partitions\n",
        "rdd_mapped_with_index = rdd.mapPartitionsWithIndex(map_partition_func_with_index)\n",
        "\n",
        "# Action: Collecting values after mapPartitionsWithIndex transformation\n",
        "mapped_values_with_index = rdd_mapped_with_index.collect()\n",
        "print(\"Mapped Values with Index (mapPartitionsWithIndex):\", mapped_values_with_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGBA21fZYKIr",
        "outputId": "78d7dbd8-06cd-4144-9b96-2dc9f26abaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped Values (mapPartitions): [3, 12]\n",
            "Mapped Values with Index (mapPartitionsWithIndex): [(0, 3), (1, 12)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Wider transformations**\n",
        "**Wider transformations, also known as shuffle-based transformations, involve data shuffling across partitions and typically result in the creation of new partitions. Unlike narrow transformations, which operate on individual partitions independently, wider transformations require data exchange and coordination across the cluster. This can incur higher computational and network overhead. Here are some common wider transformations in Apache Spark:**"
      ],
      "metadata": {
        "id": "nq5_Z6irkZOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.groupByKey:**\n",
        "\n",
        "Groups the values of an RDD with the same key into a single list.\n",
        "This transformation can lead to a significant amount of data shuffling, especially if the data associated with a single key is spread across multiple partitions.\n",
        "**2.reduceByKey:**\n",
        "\n",
        "Similar to groupByKey, but performs reduction on the values for each key.\n",
        "It combines values for each key using an associative and commutative function (like sum, max, min, etc.).\n",
        "This transformation also involves data shuffling as it needs to bring together all values for each key across partitions.\n",
        "**3.sortByKey:**\n",
        "\n",
        "Sorts the elements of an RDD by their keys.\n",
        "This transformation involves data shuffling to bring together all elements with the same key into the same partition for sorting.\n",
        "\n",
        "**4.join:**\n",
        "\n",
        "Performs an inner join between two RDDs based on their keys.\n",
        "Data from both RDDs with the same key is brought together for processing, which requires data shuffling.\n",
        "Depending on the data distribution and partitioning strategy, joining large RDDs can lead to significant shuffle overhead.\n",
        "\n",
        "**5.cogroup:**\n",
        "\n",
        "Groups the values of two RDDs with the same key into an iterator of tuples.\n",
        "This transformation requires data shuffling as it brings together values from both RDDs with the same key.\n",
        "\n",
        "**6.distinct:**\n",
        "\n",
        "Returns a new RDD containing distinct elements from the original RDD.\n",
        "Achieving distinct elements across partitions involves data shuffling to identify and eliminate duplicates.\n",
        "\n",
        "**7.union:**\n",
        "\n",
        "Concatenates two RDDs into a single RDD.\n",
        "While the transformation itself doesn't always involve shuffling, the resulting RDD may need to be re-partitioned to maintain data balance across partitions.\n",
        "\n",
        "**8.cartesian:**\n",
        "\n",
        "Computes the Cartesian product of two RDDs, generating all possible pairs of elements (one from each RDD).\n",
        "This transformation involves shuffling to combine elements from different partitions of the input RDDs.\n",
        "\n",
        "These wider transformations are powerful but should be used judiciously as they can incur significant overhead, especially when dealing with large datasets. Efficient use of partitioning and caching can help mitigate the performance impact of wider transformations in Spark applications."
      ],
      "metadata": {
        "id": "e2ZJcOJmktIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Wider Transformations Example\")\n",
        "\n",
        "# Example RDDs\n",
        "data1 = [(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"banana\", 4), (\"orange\", 5)]\n",
        "data2 = [(\"apple\", \"red\"), (\"banana\", \"yellow\"), (\"orange\", \"orange\")]\n",
        "\n",
        "# Create RDDs\n",
        "rdd1 = sc.parallelize(data1)\n",
        "rdd2 = sc.parallelize(data2)\n",
        "\n",
        "# Wider transformations\n",
        "\n",
        "# groupByKey: Group values by key\n",
        "grouped_rdd = rdd1.groupByKey().mapValues(list)\n",
        "\n",
        "# reduceByKey: Reduce values by key\n",
        "sum_by_key_rdd = rdd1.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# sortByKey: Sort RDD elements by key\n",
        "sorted_rdd = rdd2.sortByKey()\n",
        "\n",
        "# join: Inner join two RDDs based on keys\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "\n",
        "# Printing the results\n",
        "print(\"groupByKey Result:\", grouped_rdd.collect())\n",
        "print(\"reduceByKey Result:\", sum_by_key_rdd.collect())\n",
        "print(\"sortByKey Result:\", sorted_rdd.collect())\n",
        "print(\"join Result:\", joined_rdd.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CukQSmikixW",
        "outputId": "00b9162f-6ee3-4c0f-a48b-26322c1d2f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "groupByKey Result: [('apple', [1, 3]), ('banana', [2, 4]), ('orange', [5])]\n",
            "reduceByKey Result: [('apple', 4), ('banana', 6), ('orange', 5)]\n",
            "sortByKey Result: [('apple', 'red'), ('banana', 'yellow'), ('orange', 'orange')]\n",
            "join Result: [('banana', (2, 'yellow')), ('banana', (4, 'yellow')), ('orange', (5, 'orange')), ('apple', (1, 'red')), ('apple', (3, 'red'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 1: groupByKey"
      ],
      "metadata": {
        "id": "YPAI0vCBsK5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Wider Transformations Example\")\n",
        "\n",
        "# Create an RDD from a list of key-value pairs\n",
        "data = [(\"cat\", 1), (\"dog\", 2), (\"cat\", 3), (\"dog\", 4), (\"cat\", 5)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Apply groupByKey transformation to group values by key\n",
        "grouped_rdd = rdd.groupByKey()\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "print(\"Grouped RDD:\", [(key, list(values)) for key, values in grouped_rdd.collect()])\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ogtlzqDsNK6",
        "outputId": "84ac1f31-47ac-4338-e2fa-96e2e51b1188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RDD: [('cat', 1), ('dog', 2), ('cat', 3), ('dog', 4), ('cat', 5)]\n",
            "Grouped RDD: [('cat', [1, 3, 5]), ('dog', [2, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "imKKE5cksg1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 2: reduceByKey"
      ],
      "metadata": {
        "id": "Mo7WpY1ZsW3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Wider Transformations Example\")\n",
        "\n",
        "# Create an RDD from a list of key-value pairs\n",
        "data = [(\"cat\", 1), (\"dog\", 2), (\"cat\", 3), (\"dog\", 4), (\"cat\", 5)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Apply reduceByKey transformation to sum values by key\n",
        "sum_by_key_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "print(\"Reduced by Key RDD:\", sum_by_key_rdd.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi5O9FchsY4M",
        "outputId": "4c7fb369-10b8-45d8-d423-e82cc6c5b288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original RDD: [('cat', 1), ('dog', 2), ('cat', 3), ('dog', 4), ('cat', 5)]\n",
            "Reduced by Key RDD: [('cat', 9), ('dog', 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3: join"
      ],
      "metadata": {
        "id": "w1nUSE6Ystpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Wider Transformations Example\")\n",
        "\n",
        "# Create RDDs\n",
        "rdd1 = sc.parallelize([(\"cat\", 1), (\"dog\", 2), (\"cat\", 3)])\n",
        "rdd2 = sc.parallelize([(\"cat\", \"white\"), (\"dog\", \"black\"), (\"cat\", \"gray\")])\n",
        "\n",
        "# Apply join transformation to perform inner join by key\n",
        "joined_rdd = rdd1.join(rdd2)\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"RDD1:\", rdd1.collect())\n",
        "print(\"RDD2:\", rdd2.collect())\n",
        "print(\"Joined RDD:\", joined_rdd.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzVMDJ53stOd",
        "outputId": "f064b2ea-b08d-4146-f260-aab9e626e593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD1: [('cat', 1), ('dog', 2), ('cat', 3)]\n",
            "RDD2: [('cat', 'white'), ('dog', 'black'), ('cat', 'gray')]\n",
            "Joined RDD: [('cat', (1, 'white')), ('cat', (1, 'gray')), ('cat', (3, 'white')), ('cat', (3, 'gray')), ('dog', (2, 'black'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 1: Creating an RDD from a List and Performing Basic Transformations"
      ],
      "metadata": {
        "id": "Ifpk6cq0mPN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"RDD Examples\")\n",
        "\n",
        "# Create an RDD from a list\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Perform transformations\n",
        "squared_rdd = rdd.map(lambda x: x*x)\n",
        "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "print(\"Squared RDD:\", squared_rdd.collect())\n",
        "print(\"Even Numbers RDD:\", even_rdd.collect())\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "3KlR2OeDV2nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "dTn-Qll7nIt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Cartesian Product of Two RDDs"
      ],
      "metadata": {
        "id": "DaBa99EHm8Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"Cartesian Product Example\")\n",
        "\n",
        "# Create RDDs\n",
        "rdd1 = sc.parallelize([1, 2, 3])\n",
        "rdd2 = sc.parallelize(['a', 'b', 'c'])\n",
        "\n",
        "# Compute Cartesian product\n",
        "cartesian_rdd = rdd1.cartesian(rdd2)\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Cartesian Product:\")\n",
        "for pair in cartesian_rdd.collect():\n",
        "    print(pair)\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt7-6KzCm_bM",
        "outputId": "d075d5bd-c25e-44dd-f8ae-9962a57dadad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cartesian Product:\n",
            "(1, 'a')\n",
            "(1, 'b')\n",
            "(1, 'c')\n",
            "(2, 'a')\n",
            "(2, 'b')\n",
            "(2, 'c')\n",
            "(3, 'a')\n",
            "(3, 'b')\n",
            "(3, 'c')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3: Word Count using flatMap and reduceByKey"
      ],
      "metadata": {
        "id": "lXf3JF1Fmc_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fsmDg5B9m3Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Initialize SparkContext\n",
        "sc = SparkContext(\"local\", \"WordCount Example\")\n",
        "\n",
        "# Create an RDD from text file\n",
        "lines_rdd = sc.textFile(\"sample_text.txt\")\n",
        "\n",
        "# Split lines into words and flatten\n",
        "words_rdd = lines_rdd.flatMap(lambda line: line.split())\n",
        "\n",
        "# Map each word to (word, 1) tuple and reduce by key to get word count\n",
        "word_count_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Action: Collect and print results\n",
        "print(\"Word Count:\")\n",
        "for word, count in word_count_rdd.collect():\n",
        "    print(word, \": \", count)\n",
        "\n",
        "# Stop SparkContext\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "w-7bDj20m3iJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}